library(dplyr)
library(ggplot2)
library(readxl)
library(hashmap)
install.packages("hashmap")
library(dplyr)
library(ggplot2)
library(readxl)
library(hashmap)
library(dplyr)
library(ggplot2)
library(readxl)
library(hashmap)
#Empty data frame
h1b_df = data.frame()
for(year in seq(2016,2011)) {
print(paste0("Reading ", year, " records .."))
raw_data_path = paste0("./data/",year,"_raw_data.xlsx")
new_df = read_excel(raw_data_path)
print(paste0("Raw data size: ", as.character(dim(new_df))))
# Changing column names of data before 2015
print("Column matching ..")
if(year != 2015 & year != 2016){
new_df = new_df %>%
mutate(CASE_NUMBER = LCA_CASE_NUMBER,
CASE_STATUS = STATUS,
EMPLOYER_NAME = LCA_CASE_EMPLOYER_NAME,
SOC_NAME = LCA_CASE_SOC_NAME,
SOC_CODE = LCA_CASE_SOC_CODE,
JOB_TITLE = LCA_CASE_JOB_TITLE,
FULL_TIME_POSITION = FULL_TIME_POS,
PREVAILING_WAGE = PW_1,
PW_UNIT_OF_PAY = PW_UNIT_1,
WORKSITE_CITY = LCA_CASE_WORKLOC1_CITY,
WORKSITE_STATE = LCA_CASE_WORKLOC1_STATE)
}
library(dplyr)
library(ggplot2)
library(readxl)
library(hashmap)
#Empty data frame
h1b_df = read.table('h-1b-visa/h1b_kaggle.csv', header=T, as.is=T,sep=",")
library(dplyr)
library(ggplot2)
library(readxl)
library(hashmap)
#Empty data frame
h1b_df = read.csv('h-1b-visa/h1b_kaggle.csv', header=T,nrows=1000)
head(h1b_df)
library(dplyr)
library(ggplot2)
library(readxl)
library(hashmap)
#Empty data frame
h1b_df = read.csv('h-1b-visa/h1b_kaggle.csv', header=T,nrows=1000)
unique(h1b_df$CASE_STATUS)
library(dplyr)
library(ggplot2)
library(readxl)
library(hashmap)
#Empty data frame
h1b_df = read.csv('h-1b-visa/h1b_kaggle.csv', header=T,nrows=5000)
unique(h1b_df$CASE_STATUS)
library(dplyr)
library(ggplot2)
library(readxl)
library(hashmap)
#Empty data frame
h1b_df = read.csv('h-1b-visa/h1b_kaggle.csv', header=T,nrows=5000)
unique(h1b_df$CASE_STATUS)
library("RCurl")
library("XML")
library("tidyverse")
install.packages("XML")
athlete_directory <- getURL("https://www.sports-reference.com/olympics/athletes/") %>%
>>
()))
athlete_directory <- getURL("https://www.sports-reference.com/olympics/athletes/") %>%
htmlParse(asText=TRUE) %>%
xpathSApply('//td/a', xmlGetAttr, 'href') %>%
paste('https://www.sports-reference.com/', ., sep="")
library(XML)
athlete_directory <- getURL("https://www.sports-reference.com/olympics/athletes/")
athlete_directory <- getURL("https://www.sports-reference.com/olympics/athletes/") %>%
htmlParse(asText=TRUE)
head(athlete_directory)
?htmlParse
rawHtml = getURL("https://www.sports-reference.com/olympics/athletes/")
parsedHtml = htmlParse(rawHtml, asText=TRUE)
?xpathSapply
athlete_directory
xpathApply()
?xpathApply()
links = xpathSApply(parsedHtml, "//td//a", xmlGetAttr, 'href')
athlete_directory = paste(links, 'https://www.sportsreference.com', ., sep="")
athlete_directory = paste('https://www.sportsreference.com', links, sep="")
head(athlete_directory)
length(athlete_directory)
# now the next step is to go through the links and then visit the individual athlete pages
ind_links = c()
system.time(
for (i in 1:length(athlete_directory)) {
new <- getURL(athlete_directory[i]) %>%
htmlParse(asText=TRUE) %>%
xpathSApply('//*[(@id = "page_content")]//a', xmlGetAttr, 'href') %>%
paste('http://www.sports-reference.com/', ., sep="")
# update vector of athlete pages
ind_links <- c(ind_links, new)
# track progress in console
print(i)
flush.console() # avoid output buffering
}
)
# getting the directories
rawHtml = getURL("https://www.sports-reference.com/olympics/athletes/")
parsedHtml = htmlParse(rawHtml, asText=TRUE)
# <td align="center" valign="middle"><a href="/olympics/athletes/tq/">Tq</a></td>
# this is how the links are structured
links = xpathSApply(parsedHtml, "//td//a", xmlGetAttr, 'href')
athlete_directory = paste('https://www.sportsreference.com', links, sep="")
getURL("https://www.sportsreference.com/olympics/athletes/a/")
getURL("https://www.sportsreference.com/olympics/athletes/aa/")
getURL("https://www.sports-reference.com/olympics/athletes/aa/")
athlete_directory = paste('https://www.sports-reference.com', links, sep="")
# now the next step is to go through the links and then visit the individual athlete pages
ind_links = c()
system.time(
for (i in 1:length(athlete_directory)) {
new <- getURL(athlete_directory[i]) %>%
htmlParse(asText=TRUE) %>%
xpathSApply('//*[(@id = "page_content")]//a', xmlGetAttr, 'href') %>%
paste('http://www.sports-reference.com/', ., sep="")
# update vector of athlete pages
ind_links <- c(ind_links, new)
# track progress in console
print(i)
flush.console() # avoid output buffering
}
)
info_table = vector("list", length(ind_links))
results_table = vector("list", length(ind_links))
i = 1
html <- try(getURL(individual_links[i], .opts=curlOptions(followlocation=TRUE)), silent=TRUE)
head(html)
i = 1
hadstml <- try(getURL(individual_links[i], .opts=curlOptions(followlocation=TRUE)), silent=TRUE)
hadstml
i = 1
html <- try(getURL(ind_links[i], .opts=curlOptions(followlocation=TRUE)), silent=TRUE)
head(html)
html = htmlParse(html, asText=True)
html = htmlParse(html, asText=TRUE)
head(html)
xpathSApply(html, '//*[@id="info_box"]/p', xmlValue)
xpathSApply(html, '//*[@id="info_box"]/p', xmlValue) %>% strsplit("\n") %>% .[[1]]
infobox[[i]] = xpathSApply(html, '//*[@id="info_box"]/p', xmlValue) %>% strsplit("\n") %>% .[[1]]
info_table[[i]] = xpathSApply(html, '//*[@id="info_box"]/p', xmlValue) %>% strsplit("\n") %>% .[[1]]
info_table
info_table[[1]]
readHTMLTable(html) %>% .$results
# Loop through links and extract data
system.time(
for (i in 1:length(ind_links)) {
html <- try(getURL(ind_links[i], .opts=curlOptions(followlocation=TRUE)), silent=TRUE)
if(class(html) == "try-error") {
Sys.sleep(5)
html <- getURL(ind_links[i], .opts=curlOptions(followlocation=TRUE))
}
html <- htmlParse(html, asText=TRUE)
# save 'infobox'
info_table[[i]] <- xpathSApply(html, '//*[@id="info_box"]/p', xmlValue) %>%
strsplit('\n') %>% .[[1]]
# save 'results table'
results_table[[i]] <- readHTMLTable(html) %>% .$results
# track progress in console
print(i)
flush.console()
}
)
# Loop through links and extract data
system.time(
for (i in 1:10) {
html <- try(getURL(ind_links[i], .opts=curlOptions(followlocation=TRUE)), silent=TRUE)
if(class(html) == "try-error") {
Sys.sleep(5)
html <- getURL(ind_links[i], .opts=curlOptions(followlocation=TRUE))
}
html <- htmlParse(html, asText=TRUE)
# save 'infobox'
info_table[[i]] <- xpathSApply(html, '//*[@id="info_box"]/p', xmlValue) %>%
strsplit('\n') %>% .[[1]]
# save 'results table'
results_table[[i]] <- readHTMLTable(html) %>% .$results
# track progress in console
print(i)
flush.console()
}
)
# Loop through links and extract data
system.time(
for (i in 1:length(ind_links)) {
html <- try(getURL(ind_links[i], .opts=curlOptions(followlocation=TRUE)), silent=TRUE)
if(class(html) == "try-error") {
Sys.sleep(5)
html <- getURL(ind_links[i], .opts=curlOptions(followlocation=TRUE))
}
html <- htmlParse(html, asText=TRUE)
# save 'infobox'
info_table[[i]] <- xpathSApply(html, '//*[@id="info_box"]/p', xmlValue) %>%
strsplit('\n') %>% .[[1]]
# save 'results table'
results_table[[i]] <- readHTMLTable(html) %>% .$results
# track progress in console
print(i)
flush.console()
}
)
getURL
uris = c("http://www.omegahat.org/index.html", "http://www.omegahat.org/RecentActivities.html")
atime = system.time(z <- getURIs(uris))
stime = system.time(zz <- lapply(uris, getURL))
uris = c("http://www.omegahat.org/index.html", "http://www.omegahat.org/RecentActivities.html")
getURIs =
function(uris, ..., multiHandle = getCurlMultiHandle(), .perform = TRUE)
{
content = list()
curls = list()
for(i in uris) {
curl = getCurlHandle()
content[[i]] = basicTextGatherer()
opts = curlOptions(URL = i, writefunction = content[[i]]$update, ...)
curlSetOpt(.opts = opts, curl = curl)
multiHandle = push(multiHandle, curl)
}
if(.perform) {
complete(multiHandle)
lapply(content, function(x) x$value())
} else {
return(list(multiHandle = multiHandle, content = content))
}
}
atime = system.time(z <- getURIs(uris))
stime = system.time(zz <- lapply(uris, getURL))
txt = getURIAsynchronous(uris)
txt
?getURIAsynchronous
library("RCurl")
library("XML")
library("tidyverse")
# getting the directories
rawHtml = getURL("https://www.sports-reference.com/olympics/athletes/")
parsedHtml = htmlParse(rawHtml, asText=TRUE)
# <td align="center" valign="middle"><a href="/olympics/athletes/tq/">Tq</a></td>
# this is how the links are structured
links = xpathSApply(parsedHtml, "//td//a", xmlGetAttr, 'href')
athlete_directory = paste('https://www.sports-reference.com', links, sep="")
# how many are there?
#> head(athlete_directory)
#[1] "https://www.sportsreference.com/olympics/athletes/a/"  "https://www.sportsreference.com/olympics/athletes/aa/"
#[3] "https://www.sportsreference.com/olympics/athletes/ab/" "https://www.sportsreference.com/olympics/athletes/ac/"
#[5] "https://www.sportsreference.com/olympics/athletes/ad/" "https://www.sportsreference.com/olympics/athletes/ae/"
# > length(athlete_directory)
#[1] 453
# now the next step is to go through the links and then visit the individual athlete pages
ind_links = c()
system.time(
for (i in 1:length(athlete_directory)) {
new <- getURL(athlete_directory[i]) %>%
htmlParse(asText=TRUE) %>%
xpathSApply('//*[(@id = "page_content")]//a', xmlGetAttr, 'href') %>%
paste('http://www.sports-reference.com/', ., sep="")
# update vector of athlete pages
ind_links <- c(ind_links, new)
# track progress in console
print(i)
flush.console() # avoid output buffering
}
)
# user  system elapsed
# 44.58    5.64  204.07
# initializing the results variable
info_table = vector("list", length(ind_links))
results_table = vector("list", length(ind_links))
# Loop through links and extract data
system.time(
for (i in 1:length(ind_links)) {
html <- try(getURL(ind_links[i], .opts=curlOptions(followlocation=TRUE)), silent=TRUE)
if(class(html) == "try-error") {
Sys.sleep(5)
html <- getURL(ind_links[i], .opts=curlOptions(followlocation=TRUE))
}
html <- htmlParse(html, asText=TRUE)
# save 'infobox'
info_table[[i]] <- xpathSApply(html, '//*[@id="info_box"]/p', xmlValue) %>%
strsplit('\n') %>% .[[1]]
# save 'results table'
results_table[[i]] <- readHTMLTable(html) %>% .$results
# track progress in console
print(i)
flush.console()
}
)
system.time(
for (i in 1:10) {
html <- try(getURL(ind_links[i], .opts=curlOptions(followlocation=TRUE)), silent=TRUE)
if(class(html) == "try-error") {
Sys.sleep(5)
html <- getURL(ind_links[i], .opts=curlOptions(followlocation=TRUE))
}
html <- htmlParse(html, asText=TRUE)
# save 'infobox'
info_table[[i]] <- xpathSApply(html, '//*[@id="info_box"]/p', xmlValue) %>%
strsplit('\n') %>% .[[1]]
# save 'results table'
results_table[[i]] <- readHTMLTable(html) %>% .$results
# track progress in console
print(i)
flush.console()
}
)
library("RCurl")
library("XML")
library("tidyverse")
# getting the directories
rawHtml = getURL("https://www.sports-reference.com/olympics/athletes/")
parsedHtml = htmlParse(rawHtml, asText=TRUE)
# <td align="center" valign="middle"><a href="/olympics/athletes/tq/">Tq</a></td>
# this is how the links are structured
links = xpathSApply(parsedHtml, "//td//a", xmlGetAttr, 'href')
athlete_directory = paste('https://www.sports-reference.com', links, sep="")
# how many are there?
#> head(athlete_directory)
#[1] "https://www.sportsreference.com/olympics/athletes/a/"  "https://www.sportsreference.com/olympics/athletes/aa/"
#[3] "https://www.sportsreference.com/olympics/athletes/ab/" "https://www.sportsreference.com/olympics/athletes/ac/"
#[5] "https://www.sportsreference.com/olympics/athletes/ad/" "https://www.sportsreference.com/olympics/athletes/ae/"
# > length(athlete_directory)
#[1] 453
# now the next step is to go through the links and then visit the individual athlete pages
ind_links = c()
system.time(
for (i in 1:length(athlete_directory)) {
new <- getURL(athlete_directory[i]) %>%
htmlParse(asText=TRUE) %>%
xpathSApply('//*[(@id = "page_content")]//a', xmlGetAttr, 'href') %>%
paste('http://www.sports-reference.com/', ., sep="")
# update vector of athlete pages
ind_links <- c(ind_links, new)
# track progress in console
print(i)
flush.console() # avoid output buffering
}
)
# user  system elapsed
# 44.58    5.64  204.07
# initializing the results variable
info_table = vector("list", length(ind_links))
results_table = vector("list", length(ind_links))
# Loop through links and extract data
system.time(
for (i in 1:length(ind_links)) {
html <- try(getURL(ind_links[i], .opts=curlOptions(followlocation=TRUE)), silent=TRUE)
if(class(html) == "try-error") {
Sys.sleep(5)
html <- getURL(ind_links[i], .opts=curlOptions(followlocation=TRUE))
}
html <- htmlParse(html, asText=TRUE)
# save 'infobox'
info_table[[i]] <- xpathSApply(html, '//*[@id="info_box"]/p', xmlValue) %>%
strsplit('\n') %>% .[[1]]
# save 'results table'
results_table[[i]] <- readHTMLTable(html) %>% .$results
# track progress in console
print(i)
flush.console()
}
)
save(ind_links, info_table, results_table, file="scrapings.Rdata")
save(ind_links, info_table, results_table, file="scrapings.Rdata")
setwd("~/GitHub/OlympicsAthletes")
setwd("~/GitHub/OlympicsAthletes")
save(ind_links, info_table, results_table, file="scrapings.Rdata")
load('scrapings.Rdata')
n_athletes <- length(info_table) # 135584
info_table %>% lapply(function(x) grepl("Full name", x)) %>% unlist %>% sum
head(info_Table)
head(info_table)
info_table = info_table[1:50937]
results_table = results_table[1:50937
]
save(ind_links, info_table, results_table, file="scrapings_filtered.Rdata")
info <- lapply(infobox, function(x) strsplit(x[[1]], ": ")[[1]][2]) %>% unlist %>% tibble(Name = .)
info$Name %>% is.null %>% sum # check: no NULL entries
